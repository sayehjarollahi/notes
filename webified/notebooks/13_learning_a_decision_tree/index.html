---
layout: content
metadata: notebooks_13_learning_a_decision_tree_metadata
colab: https://colab.research.google.com/github/sayehjarollahi/notes/blob/master/notebooks/13_learning_a_decision_tree//index.ipynb
---

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div style="line-height:300%;">
<font face="XB Zar" size="5">
<div align="center">
<font face="IranNastaliq" size="30">
<p></p>
<p></p>
Decision Tree
                <p></p>
</font>
<font color="#FF7500">
Sharif University Of technology - Computer Engineering Depatment
            </font>
<p></p>
<font color="blue">
Artifical Intelligence - Dr. MH Rohban
            </font>
<br/>
<br/>
Spring 2021
        </div>
<hr/>
<font color="red" size="6">
<br/>
<div align="center">
Decision Tree And Its Application In Classification
                <br/>
</div>
</font>
<div align="center">
<br/>Writers Team<br/>
<font color="gray" size="6">
            Amirsadra Abdollahi<br/>
            Ashkan Khademian<br/>
            Amirmohammad Isazadeh<br/>
</font>
</div>
<hr/>
<style scoped="" type="text/css">
        p{
        border: 1px solid #a2a9b1;background-color: #f8f9fa;display: inline-block;
        };
        </style>
<div>
<h3>Contents</h3>
<ul style="margin-right: 0;">
<li>
<a href="#sec_whatisR">
                        What's Decision Tree
                    </a>
</li>
<li>
<a href="#sec_installR">
                        Decision Tree Examples
                    </a>
</li>
<li>
<a href="#sec_R_packages">
                      Building A Decision Tree
                    </a>
</li>
<li>
<a href="#sec_R_Intro">
                        Entropy And Information Gain
                    </a>
</li>
<li>
<a href="#sec_statFunc">
                        Learning Decision Tree
                    </a>
</li>
<li>
<a href="#sec_R_DSs">
                        Overfitting in Decision Trees
                    </a>
</li>
<li>
<a href="#sec_additional">
                        Additional Content
                    </a>
</li>
</ul>
</div>
</font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/></p>
<div id="sec_whatisR" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="#FF7500" size="6">
What's Decision Tree
        </font>

Decision tree learning is a method commonly used in data mining. The goal is to create a model that predicts the value of a target variable based on several input variables.
A decision tree is a simple representation for classifying examples. For this section, assume that all of the input features have finite discrete domains, and there is a single target feature called the "classification". Each element of the domain of the classification is called a class. A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature. The arcs coming from a node labeled with an input feature are labeled with each of the possible values of the target feature or the arc leads to a subordinate decision node on a different input feature. Each leaf of the tree is labeled with a class or a probability distribution over the classes, signifying that the data set has been classified by the tree into either a specific class, or into a particular probability distribution(which,if the decision tree is well-constructed, is skewed towards certain subsets of classes).
    </font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><h2>Overview</h2>
<br/>
<img src="/notes/assets/pictures/overview.png"/>
<br/></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><p></p>
<br/></p>
<div id="sec_installR" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="#FF7500" size="6">
Decision Tree Examples
        </font>
<br/><br/>
        1. Accept A New Job Offer
        <br/>
<img src="/notes/assets/pictures/find-job.png"/>
<br/>
        2. Predict Fuel Efficiency 
        <br/>
<img src="/notes/assets/pictures/fuel-efficiency.png"/>
<br/><br/>
</font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/><div id="sec_R_packages" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="#FF7500" size="6">
Building A Decision Tree
        </font>
<br/><br/>
        Now we want to build a decision tree for Fuel Efficiency Example
        <img src="/notes/assets/pictures/fuel-efficiency2.png"/>
<h2>Build decision Tree</h2>
<h3>The Starting Node</h3>
        We can start with an empty tree and improve it at each level. For example, for the second example we have a tree that for all data, it returns mpg=bad. So for our dataset we got 22 correct answers and 18 wrong answers with this tree. but we can improve it!
        <img src="/notes/assets/pictures/fuel-efficiency3.png" style="width:200px;height:100px"/></font></div></p>
<p>&lt;/div&gt;</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div style="line-height:300%;">
<font face="XB Zar" size="5">
<h2>Operators</h2>
        There are several operators for building a decision tree.
        <h4>Improving the tree</h4>
        To improve our tree, we can add more nodes(features) to get more correct answer and predict better.
        <img src="/notes/assets/pictures/fuel-efficiency4.png"/>
<h4>Recursive step</h4>
        We can consider each leaf as a root and do the same for those.Maybe we get to some node that doesn’t have any data. In these situations we should end up and predict randomly.
        <img src="/notes/assets/pictures/fuel-efficiency5.png">
        After one level we got this tree ?
        <img src="/notes/assets/pictures/fuel-efficiency6.png"/>
        And after adding all nodes(features) to the tree we got a Full Tree like this:
        <img src="/notes/assets/pictures/fuel-efficiency7.png"/>
</img></font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="sec_R_Intro" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="#FF7500" size="7">
        Two Questions For More Efficiency
        </font>
<p></p>
<hr/>
<h3>Hill Climbing Algorithm:</h3>

      •Start from empty decision tree
      •Split on the best attribute (feature) – Recurse
 Now the important question is:

      •Which attribute gives the best split? 
      •When to stop recursion? 

</font></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="sec_R_Intro" style="line-height:300%;">
<font face="XB Zar" size="5">
<h3>Splitting: choosing a good attribute</h3>

Look at the example to get the main point of splitting! 
<img src="/notes/assets/pictures/p10.png"/>
<h4>Measuring uncertainty</h4>
<br/>
•Good split if we are more certain about classification after split<br/>
     •Deterministic good (all true or all false)<br/>
     •Uniform distribution?   BAD<br/>
</font></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="sec_whatisR" style="line-height:300%;">
<font face="XB Zar" size="5">
<h3>Which attribute gives the best split? </h3>
        • A1: The one with the highest information gain<br/>
       • Defined in terms of entropy<br/>
 • A2: Actually many alternatives,<br/>
      • e.g., accuracy. Seeks to reduce the misclassification rate

</font></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/></p>
<div id="sec_R_Intro" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="#FF7500" size="7">
        Entropy And Information Gain
        </font>
<p></p>
<hr/>
<font color="purple" size="6">Entropy</font><br/>
        First we start with definition of entropy:<br/>
      Entropy H(Y) of a random variable Y:
<img src="/notes/assets/pictures/p16.png" style="width:400px"/>
As it can be figured out, more uncertainty, more entropy!<br/>
It's interesting to know that in Information Theory interpretation entropy is defined as:<br/>
    Expected number of bits needed to encode a randomly drawn value of Y (under most efficient code)<br/>
And for a diagram view of entropy we can say it has a peak in the middle of it, then it falls.
<img src="/notes/assets/pictures/p17.png" style="height:300px"/>
<font color="green" size="5.7">Example of Entropy</font><br/>
If P(Y=t) = 5/6 and P(Y=f) = 1/6 the value of H(Y) would be:
<img src="/notes/assets/pictures/p18.png" style="height:300px"/>
<font color="purple" size="6">Conditional Entropy</font><br/>
definition:<br/>
   Conditional Entropy H(Y|X) of a random variable Y conditioned on a random
variable X<br/>
<img src="/notes/assets/pictures/p19.png"/>
And here an example of conditional entropy:<br/>
<img src="/notes/assets/pictures/p20.png" style="height:300px"/>
</font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/></p>
<div style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="purple" size="6.6">Information Gain</font><br/>
        definition:<br/>
    The information gain is the amount of information gained about a random variable or signal from observing another random variable and defined by this equation:
<img src="/notes/assets/pictures/p21.png"/>
And for IG of our last example we have:
<img src="/notes/assets/pictures/p22.png"/>
</font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/></p>
<div id="sec_statFunc" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="#FF7500" size="6">
        Learning Decision Tree
        </font><br/>
        For learning a decision tree we should consider some important things such as:<br/>
      . Select a node for splitting<br/>
      . Update tree after an iteration<br/>
      . Terminating condition<br/>

Steps of building a tree:<br/>
      . Start from empty decision tree<br/>
      . Split on next best attribute (feature)<br/>
      . Use information gain (or...?) to select attribute:<br/>
<img src="/notes/assets/pictures/p23.png" style="width:400px"/>
      . Recurse<br/><br/>
<font color="green" size="6">Example</font><br/>       
Now we want to build a tree step by step, as an example for figuring out the way to build a DT.<br/>
Suppose we want to predict MPG; now look at all the information gains in the picture below
<img src="/notes/assets/pictures/p24.png"/>
Now we should do our iterations.<br/>
For first iteration we have:
<img src="/notes/assets/pictures/p25.png"/>
</font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/></p>
<div id="sec_statFunc" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="purple" size="6">
        Termination Conditions
        </font><br/>
        We introduce conditions to know when to stop building process.<br/>
        Two ideas for terminating are:<br/>
      . Base Case One: If all records in current data subset have the same output then
don’t recurse.<br/>
      . Base Case Two: If all records have exactly the same set of input attributes then
don’t recurse.<br/>
Base Case One:
        <img src="/notes/assets/pictures/p26.png"/>
Base Case Two:
        <img src="/notes/assets/pictures/p27.png"/>
<br/>
Now there is one other idea for termination that we want to discuss:<br/>
    . Base Case 3: If all attributes have zero information gain then don't recurse.<br/>
<font color="red" size="5">
        Is it a good idea??
        </font><br/>
    The answer is no!!<br/>
Because it's kind of a greedy idea!!<br/>
In this idea we check information gain of our variables alone, but if we check the information we get from combination of variables, there maybe be usefull information.<br/>
Here an example of this idea's problem:<br/>
    For Y = a xor b, we use this idea and here the result:
<img src="/notes/assets/pictures/p28.png"/>
But without this idea:
<img src="/notes/assets/pictures/p29.png"/>
<img src="/notes/assets/pictures/p30.png"/>
<img src="/notes/assets/pictures/p31.png"/>
So as we can see, this idea didn't come up with good results.
</font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/></p>
<div id="sec_R_DSs" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="#FF7500" size="6">
            Overfitting in Decision Trees
        </font>
<h4>How to resolve Overfitting ?</h4>

Decision trees have no learning bias and in outcome, the variance over the training set is significantly high. How can we introduce some biases to resolve the overfitting?
        <h5>No Free Lunch Theorem</h5>
Take the facing graph as an example. Suppose we are given a dataset of (x, y)s and we want to estimate the function which caused these points.
As you might’ve gotten it so far, all the three functions (red, green, and blue) are estimating it right and if we don’t have any prior knowledge, all three functions have the exact probability to be our suggesting function.
“No Free lunch” states that, in absence of any sense on what functions are more likely, learning is impossible.
For example, if we already knew that a very smooth function generated these points we are more likely to prefer the blue function to others.
<img src="/notes/assets/pictures/p11.png" style="width:400px;height:300px"/>
</font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/></p>
<div id="sec_whatisR" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="#000000" size="5">
<b>
<font color="gray" size="6">
                Occam’s Razor :
                    </font>
</b>
</font>
<br/>
        This principle states that from all possible hypotheses on a dataset, choose the shortest (the least complex) one. Because a short hypothesis is less likely to overfit.
        <img src="/notes/assets/pictures/p12.png" style="width:700px;height:300px"/>
<h6>Variance of model</h6>
        If we change the training data, how much our result on validation set changes
        <h6>Bias of model</h6>
        The deviation we took from the original training data while we were training
        <br/>
        According to this principle we’d rather make smaller trees with less depth.

</font></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="sec_whatisR" style="line-height:300%;">
<font face="XB Zar" size="5">
<b>
<font color="brown" size="6.5">How To Build Small Trees</font>
</b>
<br/>
        There are some approaches as followed:
        <br/>
<br/>
<font color="purple" size="6">Stop growing tree before overfit<br/></font>
      . Bound depth or #leaves<br/>
      . Stop growing more depth if in one depth Information Gain (IG) of all<br/>
      . features on remaining data is zero.<br/>
<font color="purple" size="6">Grow full tree then prune<br/></font>
      . Optimize on a held-out (validation set). Meaning after the training is finished prune any branches that do not reduce the accuracy on validation set below a threshold. (Con: requires a larger amount of data)<br/>
      . Use statistical significance testing<br/>
</font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div style="line-height:300%;">
<font face="XB Zar" size="5">
<b>
<font color="brown" size="6.5">Use statistical significance testing</font>
</b>
<font color="purple" size="6">Chi-Square Test (remembrance)<br/></font>
Having two features where the first feature has k classes and the second one has n classes. With people which we know their classes in both features, H0(null hypothesis) states that in a table with k rows and n columns, rows are independent from columns and conversely.
        <br/>
<img src="/notes/assets/pictures/p13.png"/>
<font color="purple" size="6">Using Chi-Square in growing tree<br/></font>
As you know, for choosing what feature to be used in each depth we got help from a concept called IG. after the tree is grown (fully or with a limit on depth and leaves) we test each feature in each depth and calculate the p_value. After setting a threshold calledMaxPchance, if calculate pfor any feature is greater than MaxPchance. We prune that feature and the decision will be based on the previous feature (parent node).
How to find a good MaxPchance?
We could use a local or greedy search on the validation set and by reaching a good accuracy on the validation set we would stop and set that MaxPchance.


</font>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br/></p>
<div id="sec_additional" style="line-height:300%;">
<font face="XB Zar" size="5">
<font color="#FF7500" size="7">
Additional Content<br/>
</font>
<font color="purple" size="6.5">Random Forests<br/></font>
Idea:<br/> 
     Decision Trees overfit easily and it's hard to grow them on a big number of features.<br/>
Approach:<br/> 
     Use multiple decision trees instead of only one.
<img src="/notes/assets/pictures/p14.png"/>
For each tree:<br/>
<font color="green" size="6">Training Set</font><br/>
Assume T as the original training set. It’s not usual to divide it into N subsets (N number of decision trees in forest). For  the sake of resolving the overfit, we choose a subset size of $φ1|T|$ then we bootstrap (bagging) T into N subsets of size $φ1|T|$. ($0 &lt; φ1|T| &lt;=1$)<br/>
<font color="green" size="6">Features</font><br/>
One Problem of a single decision tree was that it was hard to grow it on a lot of features. Then here we choose randomly a subset of features with size $φ2|F|$ and assign it to each tree in the forest. ($0 &lt; φ2|F| &lt;=1$)<br/>
<font color="green" size="6">Prediction</font><br/>
There are two methods to report the final prediction of each sample of the test set.<br/>
<font color="purple" size="5.5">1. Max Method</font><br/>
In this method we report the class which is predicted the most. for example if 3 out of 5 trees said the class is  𝑌=1 we report  𝑌=1 . properties:<br/>
      . less F1 score<br/>
      . more recall<br/>
      . less precision<br/>
      . should choose randomly when the number of trees is even<br/>
<font color="purple" size="5.5">2. Avg Method</font><br/>
As we know each tree has a probability with its prediction which shows how much does he persist that its answer is true. We calculate a weighted average of each tree's prediction where the weights are the probability reported from each tree alongside its prediction.<br/> properties:<br/>
      . more F1<br/>
      . less recall<br/>
      . more precision<br/>
      . has a more reasonable answer when the number of trees is even<br/>
<img src="/notes/assets/pictures/p15.png"/>
</font>
</div>
</div>
</div>
</div>
